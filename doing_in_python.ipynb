{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import stupidf as sf\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "def extract_scaling_info_and_apply(stdf_file):\n",
    "    \"\"\"\n",
    "    Extract scaling information from raw STDF and apply it to create scaled columns\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Loading STDF data...\")\n",
    "    \n",
    "    # Get the regular DataFrame (this has the test results)\n",
    "    stdf_parsed = sf.parse_stdf(stdf_file)\n",
    "    df = stdf_parsed['df']\n",
    "    \n",
    "    # Get the raw STDF object (this has detailed PTR info including scaling)\n",
    "    raw_stdf = sf.get_raw_stdf(stdf_file)\n",
    "    \n",
    "    print(f\"DataFrame shape: {df.shape}\")\n",
    "    print(f\"DataFrame columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Extract scaling information from the full test information\n",
    "    full_test_info = raw_stdf.test_data.full_test_information\n",
    "    \n",
    "    print(f\"Found {len(full_test_info)} test information records\")\n",
    "    \n",
    "    # Create dictionaries to store scaling info by test_num\n",
    "    scaling_info = {}\n",
    "    \n",
    "    # Extract scaling data from each test information record\n",
    "    for (test_num, site_num, head_num), test_info in full_test_info.items():\n",
    "        if test_num not in scaling_info:\n",
    "            scaling_info[test_num] = {\n",
    "                'units': getattr(test_info, 'units', ''),\n",
    "                'res_scal': None,\n",
    "                'llm_scal': None, \n",
    "                'hlm_scal': None,\n",
    "                'low_limit': getattr(test_info, 'low_limit', None),\n",
    "                'high_limit': getattr(test_info, 'high_limit', None)\n",
    "            }\n",
    "    \n",
    "    print(f\"Extracted scaling info for {len(scaling_info)} unique tests\")\n",
    "    \n",
    "    # Alternative: Parse the raw records directly to get PTR scaling info\n",
    "    # This is more reliable since the test_information might not have all scaling data\n",
    "    print(\"Parsing raw records for PTR scaling information...\")\n",
    "    \n",
    "    ptr_scaling_info = extract_ptr_scaling_from_raw(stdf_file)\n",
    "    \n",
    "    # Merge the two sources of information\n",
    "    for test_num in ptr_scaling_info:\n",
    "        if test_num in scaling_info:\n",
    "            scaling_info[test_num].update(ptr_scaling_info[test_num])\n",
    "        else:\n",
    "            scaling_info[test_num] = ptr_scaling_info[test_num]\n",
    "    \n",
    "    print(f\"Total scaling info for {len(scaling_info)} tests\")\n",
    "    \n",
    "    # Convert DataFrame to pandas for easier manipulation\n",
    "    df_pandas = df.to_pandas()\n",
    "    \n",
    "    # Find parametric test columns (numeric column names)\n",
    "    param_columns = [col for col in df_pandas.columns if col.isdigit()]\n",
    "    print(f\"Found {len(param_columns)} parametric test columns: {param_columns[:10]}\")\n",
    "    \n",
    "    # Apply scaling to each parametric test\n",
    "    new_columns = {}\n",
    "    \n",
    "    for test_num_str in param_columns:\n",
    "        test_num = int(test_num_str)\n",
    "        \n",
    "        if test_num in scaling_info:\n",
    "            info = scaling_info[test_num]\n",
    "            \n",
    "            # Get the raw results column\n",
    "            raw_values = df_pandas[test_num_str]\n",
    "            \n",
    "            # Apply scaling if res_scal is available\n",
    "            if info.get('res_scal') is not None:\n",
    "                res_scal = info['res_scal']\n",
    "                scaled_values = raw_values * (10 ** res_scal)\n",
    "                new_columns[f\"{test_num}_scaled\"] = scaled_values\n",
    "                new_columns[f\"{test_num}_res_scal\"] = res_scal\n",
    "                print(f\"Test {test_num}: Applied scaling 10^{res_scal}\")\n",
    "            else:\n",
    "                # No scaling, but still create scaled column (same as raw)\n",
    "                new_columns[f\"{test_num}_scaled\"] = raw_values\n",
    "                new_columns[f\"{test_num}_res_scal\"] = 0\n",
    "            \n",
    "            # Add other scaling info\n",
    "            if info.get('llm_scal') is not None:\n",
    "                new_columns[f\"{test_num}_llm_scal\"] = info['llm_scal']\n",
    "            \n",
    "            if info.get('hlm_scal') is not None:\n",
    "                new_columns[f\"{test_num}_hlm_scal\"] = info['hlm_scal']\n",
    "            \n",
    "            # Add units\n",
    "            units = info.get('units', '')\n",
    "            if units:\n",
    "                new_columns[f\"{test_num}_units\"] = units\n",
    "            \n",
    "            # Add scaled limits if available\n",
    "            if info.get('low_limit') is not None and info.get('llm_scal') is not None:\n",
    "                llm_scal = info['llm_scal']\n",
    "                scaled_low_limit = info['low_limit'] * (10 ** llm_scal)\n",
    "                new_columns[f\"{test_num}_low_limit_scaled\"] = scaled_low_limit\n",
    "                \n",
    "            if info.get('high_limit') is not None and info.get('hlm_scal') is not None:\n",
    "                hlm_scal = info['hlm_scal']\n",
    "                scaled_high_limit = info['high_limit'] * (10 ** hlm_scal)\n",
    "                new_columns[f\"{test_num}_high_limit_scaled\"] = scaled_high_limit\n",
    "    \n",
    "    # Add new columns to DataFrame\n",
    "    for col_name, col_data in new_columns.items():\n",
    "        df_pandas[col_name] = col_data\n",
    "    \n",
    "    print(f\"Added {len(new_columns)} new scaling columns\")\n",
    "    \n",
    "    # Convert back to Polars if needed\n",
    "    enhanced_df = pl.from_pandas(df_pandas)\n",
    "    \n",
    "    return enhanced_df, scaling_info\n",
    "\n",
    "def extract_ptr_scaling_from_raw(stdf_file):\n",
    "    \"\"\"\n",
    "    Alternative method: Parse the file again and extract PTR records directly\n",
    "    This gives us access to the scaling fields that are already parsed\n",
    "    \"\"\"\n",
    "    \n",
    "    # Import the internal stupidf modules to access raw records\n",
    "    try:\n",
    "        # This is a bit hacky but should work to get raw PTR records\n",
    "        from stupidf.records import Records\n",
    "        from stupidf.records.records import Record\n",
    "        \n",
    "        ptr_scaling = {}\n",
    "        \n",
    "        # Parse the file record by record\n",
    "        records = Records(stdf_file)\n",
    "        \n",
    "        for record in records:\n",
    "            if resolved := record.resolve():\n",
    "                if isinstance(resolved, Record) and hasattr(resolved, '__class__'):\n",
    "                    # Check if it's a PTR record\n",
    "                    if resolved.__class__.__name__ == 'PTR' or str(type(resolved)).endswith('PTR'):\n",
    "                        ptr = resolved\n",
    "                        test_num = ptr.test_num\n",
    "                        \n",
    "                        ptr_scaling[test_num] = {\n",
    "                            'res_scal': getattr(ptr, 'res_scal', 0),\n",
    "                            'llm_scal': getattr(ptr, 'llm_scal', 0), \n",
    "                            'hlm_scal': getattr(ptr, 'hlm_scal', 0),\n",
    "                            'units': getattr(ptr, 'units', ''),\n",
    "                            'low_limit': getattr(ptr, 'lo_limit', None),\n",
    "                            'high_limit': getattr(ptr, 'hi_limit', None)\n",
    "                        }\n",
    "        \n",
    "        print(f\"Extracted PTR scaling info for {len(ptr_scaling)} tests\")\n",
    "        return ptr_scaling\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"Could not import internal records module, using alternative method...\")\n",
    "        return extract_ptr_scaling_alternative(stdf_file)\n",
    "\n",
    "def extract_ptr_scaling_alternative(stdf_file):\n",
    "    \"\"\"\n",
    "    Alternative method using only the public API\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get rows which contain individual test results\n",
    "    rows = sf.get_rows(stdf_file)\n",
    "    \n",
    "    # This won't give us scaling info directly, but we can try to get it\n",
    "    # from the test information\n",
    "    stdf_parsed = sf.parse_stdf(stdf_file)\n",
    "    test_info = stdf_parsed['test_information'].to_pandas()\n",
    "    \n",
    "    ptr_scaling = {}\n",
    "    \n",
    "    # Extract what we can from test information\n",
    "    for _, row in test_info.iterrows():\n",
    "        if row['test_type'] == 'P':  # Parametric test\n",
    "            test_num = row['test_num']\n",
    "            ptr_scaling[test_num] = {\n",
    "                'res_scal': 0,  # Default, since not available in merged test info\n",
    "                'llm_scal': 0,\n",
    "                'hlm_scal': 0, \n",
    "                'units': row.get('units', ''),\n",
    "                'low_limit': row.get('low_limit', None),\n",
    "                'high_limit': row.get('high_limit', None)\n",
    "            }\n",
    "    \n",
    "    return ptr_scaling\n",
    "\n",
    "def analyze_scaling_results(enhanced_df, scaling_info):\n",
    "    \"\"\"\n",
    "    Analyze the results of the scaling operation\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"SCALING ANALYSIS RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    df_pandas = enhanced_df.to_pandas()\n",
    "    \n",
    "    # Find all scaling-related columns\n",
    "    scaled_cols = [col for col in df_pandas.columns if '_scaled' in col]\n",
    "    res_scal_cols = [col for col in df_pandas.columns if '_res_scal' in col]\n",
    "    units_cols = [col for col in df_pandas.columns if '_units' in col]\n",
    "    \n",
    "    print(f\"Created {len(scaled_cols)} scaled result columns\")\n",
    "    print(f\"Created {len(res_scal_cols)} scaling exponent columns\")\n",
    "    print(f\"Created {len(units_cols)} units columns\")\n",
    "    \n",
    "    # Show some examples\n",
    "    print(f\"\\nExample scaled columns: {scaled_cols[:5]}\")\n",
    "    print(f\"Example units columns: {units_cols[:5]}\")\n",
    "    \n",
    "    # Analyze scaling factors used\n",
    "    scaling_factors_used = set()\n",
    "    units_found = set()\n",
    "    \n",
    "    for test_num, info in scaling_info.items():\n",
    "        if info.get('res_scal') is not None:\n",
    "            scaling_factors_used.add(info['res_scal'])\n",
    "        if info.get('units'):\n",
    "            units_found.add(info['units'])\n",
    "    \n",
    "    print(f\"\\nScaling factors found: {sorted(scaling_factors_used)}\")\n",
    "    print(f\"Units found: {sorted(units_found)}\")\n",
    "    \n",
    "    # Show a detailed example for one test\n",
    "    if scaled_cols:\n",
    "        example_test = scaled_cols[0].replace('_scaled', '')\n",
    "        raw_col = example_test\n",
    "        scaled_col = f\"{example_test}_scaled\"\n",
    "        scal_col = f\"{example_test}_res_scal\"\n",
    "        \n",
    "        if all(col in df_pandas.columns for col in [raw_col, scaled_col, scal_col]):\n",
    "            print(f\"\\nDetailed example for test {example_test}:\")\n",
    "            \n",
    "            sample_data = df_pandas[[raw_col, scaled_col, scal_col]].dropna().head(5)\n",
    "            \n",
    "            for idx, row in sample_data.iterrows():\n",
    "                raw = row[raw_col]\n",
    "                scaled = row[scaled_col]\n",
    "                scal = row[scal_col]\n",
    "                \n",
    "                expected = raw * (10 ** scal) if scal != 0 else raw\n",
    "                print(f\"  Row {idx}: {raw:.6f} × 10^{scal} = {expected:.6f} (got {scaled:.6f})\")\n",
    "    \n",
    "    return enhanced_df\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to demonstrate the scaling workaround\n",
    "    \"\"\"\n",
    "    \n",
    "    # Replace with your STDF file path\n",
    "    stdf_file = \"your_file.stdf\"  # Change this!\n",
    "    \n",
    "    try:\n",
    "        # Extract scaling info and create enhanced DataFrame\n",
    "        enhanced_df, scaling_info = extract_scaling_info_and_apply(stdf_file)\n",
    "        \n",
    "        # Analyze the results\n",
    "        final_df = analyze_scaling_results(enhanced_df, scaling_info)\n",
    "        \n",
    "        print(f\"\\n✅ Successfully created enhanced DataFrame with scaling!\")\n",
    "        print(f\"   Original columns: {len(sf.parse_stdf(stdf_file)['df'].columns)}\")\n",
    "        print(f\"   Enhanced columns: {len(final_df.columns)}\")\n",
    "        \n",
    "        # Save the enhanced DataFrame if desired\n",
    "        # final_df.write_csv(\"enhanced_stdf_data.csv\")\n",
    "        \n",
    "        return final_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Quick test function you can run\n",
    "    def quick_test(stdf_file):\n",
    "        \"\"\"Quick test to see if scaling info is available\"\"\"\n",
    "        \n",
    "        print(f\"Testing scaling extraction for: {stdf_file}\")\n",
    "        \n",
    "        # Get raw data\n",
    "        raw = sf.get_raw_stdf(stdf_file)\n",
    "        \n",
    "        # Check test information\n",
    "        full_test_info = raw.test_data.full_test_information\n",
    "        \n",
    "        print(f\"Found {len(full_test_info)} test information records\")\n",
    "        \n",
    "        # Check a few records for scaling info\n",
    "        sample_tests = list(full_test_info.items())[:3]\n",
    "        \n",
    "        for (test_num, site, head), test_info in sample_tests:\n",
    "            print(f\"\\nTest {test_num} (site {site}, head {head}):\")\n",
    "            print(f\"  Units: {getattr(test_info, 'units', 'N/A')}\")\n",
    "            print(f\"  Low limit: {getattr(test_info, 'low_limit', 'N/A')}\")\n",
    "            print(f\"  High limit: {getattr(test_info, 'high_limit', 'N/A')}\")\n",
    "    \n",
    "    # Uncomment and modify to test:\n",
    "    # quick_test(\"your_file.stdf\")\n",
    "    # main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import stupidf as sf\n",
    "import pandas as pd\n",
    "\n",
    "def investigate_stdf_structure(stdf_file):\n",
    "    \"\"\"\n",
    "    First, let's see what's actually available in the STDF data\n",
    "    \"\"\"\n",
    "    print(\"🔍 Investigating STDF structure...\")\n",
    "    \n",
    "    try:\n",
    "        # Get parsed data\n",
    "        stdf_parsed = sf.parse_stdf(stdf_file)\n",
    "        print(\"✅ Successfully parsed STDF file\")\n",
    "        \n",
    "        # Check main dataframe\n",
    "        df = stdf_parsed['df']\n",
    "        print(f\"📊 Main DataFrame: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "        \n",
    "        # Check test information\n",
    "        test_info = stdf_parsed['test_information']\n",
    "        print(f\"📋 Test Information: {test_info.shape[0]} tests\")\n",
    "        print(f\"   Columns: {list(test_info.columns)}\")\n",
    "        \n",
    "        # Check what's in the parsed dict\n",
    "        print(f\"🗂️  Parsed STDF keys: {list(stdf_parsed.keys())}\")\n",
    "        \n",
    "        # Get raw STDF\n",
    "        raw_stdf = sf.get_raw_stdf(stdf_file)\n",
    "        print(f\"🔧 Raw STDF type: {type(raw_stdf)}\")\n",
    "        \n",
    "        if isinstance(raw_stdf, dict):\n",
    "            print(f\"   Raw STDF keys: {list(raw_stdf.keys())}\")\n",
    "            \n",
    "            # Check test_data structure\n",
    "            if 'test_data' in raw_stdf:\n",
    "                test_data = raw_stdf['test_data']\n",
    "                print(f\"   Test data type: {type(test_data)}\")\n",
    "                \n",
    "                # If test_data is an object, check its attributes\n",
    "                if hasattr(test_data, '__dict__'):\n",
    "                    print(f\"   Test data attributes: {list(test_data.__dict__.keys())}\")\n",
    "                elif hasattr(test_data, '__dir__'):\n",
    "                    attrs = [attr for attr in dir(test_data) if not attr.startswith('_')]\n",
    "                    print(f\"   Test data methods/attrs: {attrs}\")\n",
    "        \n",
    "        return stdf_parsed, raw_stdf\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error investigating structure: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def extract_available_scaling_info(stdf_file):\n",
    "    \"\"\"\n",
    "    Extract whatever scaling information is available using the correct access patterns\n",
    "    \"\"\"\n",
    "    print(\"\\n🔧 Extracting available scaling information...\")\n",
    "    \n",
    "    try:\n",
    "        # Get the parsed data - this is what definitely works\n",
    "        stdf_parsed = sf.parse_stdf(stdf_file)\n",
    "        df = stdf_parsed['df'].to_pandas()\n",
    "        test_info = stdf_parsed['test_information'].to_pandas()\n",
    "        \n",
    "        print(f\"📊 Working with {len(df)} data rows and {len(test_info)} test definitions\")\n",
    "        \n",
    "        # Find parametric test columns\n",
    "        param_columns = [col for col in df.columns if col.isdigit()]\n",
    "        print(f\"🧮 Found {len(param_columns)} parametric test columns\")\n",
    "        \n",
    "        # Extract metadata from test information\n",
    "        scaling_metadata = {}\n",
    "        \n",
    "        for _, test_row in test_info.iterrows():\n",
    "            test_num = test_row['test_num']\n",
    "            test_type = test_row.get('test_type', '')\n",
    "            \n",
    "            if test_type == 'P':  # Parametric test\n",
    "                scaling_metadata[test_num] = {\n",
    "                    'test_name': test_row.get('test_name', ''),\n",
    "                    'units': test_row.get('units', ''),\n",
    "                    'low_limit': test_row.get('low_limit', None),\n",
    "                    'high_limit': test_row.get('high_limit', None),\n",
    "                    'test_text': test_row.get('test_text', ''),\n",
    "                }\n",
    "        \n",
    "        print(f\"📋 Extracted metadata for {len(scaling_metadata)} parametric tests\")\n",
    "        \n",
    "        # Add metadata columns to the main dataframe\n",
    "        new_columns = {}\n",
    "        \n",
    "        for col in param_columns:\n",
    "            test_num = int(col)\n",
    "            \n",
    "            if test_num in scaling_metadata:\n",
    "                metadata = scaling_metadata[test_num]\n",
    "                \n",
    "                # Add units\n",
    "                if metadata['units']:\n",
    "                    new_columns[f\"{col}_units\"] = metadata['units']\n",
    "                \n",
    "                # Add test name\n",
    "                if metadata['test_name']:\n",
    "                    new_columns[f\"{col}_test_name\"] = metadata['test_name']\n",
    "                \n",
    "                # Add limits\n",
    "                if metadata['low_limit'] is not None and pd.notna(metadata['low_limit']):\n",
    "                    new_columns[f\"{col}_low_limit\"] = metadata['low_limit']\n",
    "                \n",
    "                if metadata['high_limit'] is not None and pd.notna(metadata['high_limit']):\n",
    "                    new_columns[f\"{col}_high_limit\"] = metadata['high_limit']\n",
    "                \n",
    "                # Add test description\n",
    "                if metadata['test_text']:\n",
    "                    new_columns[f\"{col}_description\"] = metadata['test_text']\n",
    "        \n",
    "        # Add new columns to dataframe\n",
    "        for col_name, col_value in new_columns.items():\n",
    "            df[col_name] = col_value\n",
    "        \n",
    "        print(f\"✅ Added {len(new_columns)} metadata columns\")\n",
    "        \n",
    "        return df, scaling_metadata\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error extracting scaling info: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "def try_access_raw_test_data(stdf_file):\n",
    "    \"\"\"\n",
    "    Attempt to access the raw test data to get PTR scaling information\n",
    "    \"\"\"\n",
    "    print(\"\\n🔍 Attempting to access raw test data...\")\n",
    "    \n",
    "    try:\n",
    "        raw_stdf = sf.get_raw_stdf(stdf_file)\n",
    "        \n",
    "        if isinstance(raw_stdf, dict) and 'test_data' in raw_stdf:\n",
    "            test_data = raw_stdf['test_data']\n",
    "            \n",
    "            # Try different ways to access the data\n",
    "            if hasattr(test_data, 'full_test_information'):\n",
    "                full_test_info = test_data.full_test_information\n",
    "                print(f\"✅ Found full_test_information with {len(full_test_info)} records\")\n",
    "                \n",
    "                # Sample a few records to see what's available\n",
    "                sample_records = list(full_test_info.items())[:3]\n",
    "                \n",
    "                for (test_num, site_num, head_num), test_info in sample_records:\n",
    "                    print(f\"\\n📝 Test {test_num} (site {site_num}, head {head_num}):\")\n",
    "                    \n",
    "                    # Try to access scaling fields\n",
    "                    attrs_to_check = ['units', 'low_limit', 'high_limit', 'res_scal', 'llm_scal', 'hlm_scal']\n",
    "                    \n",
    "                    for attr in attrs_to_check:\n",
    "                        if hasattr(test_info, attr):\n",
    "                            value = getattr(test_info, attr)\n",
    "                            print(f\"   {attr}: {value}\")\n",
    "                \n",
    "                return full_test_info\n",
    "                \n",
    "            else:\n",
    "                print(\"❌ Could not access full_test_information\")\n",
    "                return None\n",
    "        else:\n",
    "            print(\"❌ Could not access test_data from raw STDF\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error accessing raw test data: {e}\")\n",
    "        return None\n",
    "\n",
    "def apply_known_scaling_patterns(df):\n",
    "    \"\"\"\n",
    "    Apply common scaling patterns based on typical STDF usage\n",
    "    \"\"\"\n",
    "    print(\"\\n🔧 Applying common scaling patterns...\")\n",
    "    \n",
    "    # Common scaling patterns in semiconductor testing\n",
    "    scaling_patterns = {\n",
    "        'voltage': {'keywords': ['volt', 'vdd', 'vcc', 'supply'], 'likely_scales': [-3, -6, -9]},  # mV, µV, nV\n",
    "        'current': {'keywords': ['curr', 'idd', 'icc', 'leak'], 'likely_scales': [-3, -6, -9, -12]},  # mA, µA, nA, pA\n",
    "        'resistance': {'keywords': ['res', 'ohm'], 'likely_scales': [0, 3, 6]},  # Ω, kΩ, MΩ\n",
    "        'frequency': {'keywords': ['freq', 'hz'], 'likely_scales': [3, 6, 9]},  # kHz, MHz, GHz\n",
    "        'time': {'keywords': ['time', 'delay', 'period'], 'likely_scales': [-3, -6, -9, -12]},  # ms, µs, ns, ps\n",
    "    }\n",
    "    \n",
    "    # Find columns with units information\n",
    "    units_cols = [col for col in df.columns if '_units' in col]\n",
    "    \n",
    "    scaling_applied = 0\n",
    "    \n",
    "    for units_col in units_cols:\n",
    "        test_num_col = units_col.replace('_units', '')\n",
    "        \n",
    "        if test_num_col in df.columns:\n",
    "            # Get the units value (assuming it's constant)\n",
    "            units_values = df[units_col].dropna().unique()\n",
    "            \n",
    "            if len(units_values) > 0:\n",
    "                units = str(units_values[0]).lower()\n",
    "                \n",
    "                # Check for scaling patterns\n",
    "                for pattern_name, pattern_info in scaling_patterns.items():\n",
    "                    for keyword in pattern_info['keywords']:\n",
    "                        if keyword in units:\n",
    "                            print(f\"   🎯 Found {pattern_name} pattern in test {test_num_col}: {units}\")\n",
    "                            \n",
    "                            # Apply most likely scaling\n",
    "                            if pattern_info['likely_scales']:\n",
    "                                scale = pattern_info['likely_scales'][0]  # Use most common scale\n",
    "                                \n",
    "                                scaled_values = df[test_num_col] * (10 ** scale)\n",
    "                                df[f\"{test_num_col}_scaled_{pattern_name}\"] = scaled_values\n",
    "                                df[f\"{test_num_col}_scale_factor\"] = scale\n",
    "                                \n",
    "                                print(f\"     Applied 10^{scale} scaling\")\n",
    "                                scaling_applied += 1\n",
    "                            break\n",
    "    \n",
    "    print(f\"✅ Applied scaling to {scaling_applied} tests based on common patterns\")\n",
    "    return df\n",
    "\n",
    "def main_scaling_workflow(stdf_file):\n",
    "    \"\"\"\n",
    "    Main workflow to extract and apply scaling information\n",
    "    \"\"\"\n",
    "    print(\"🚀 Starting scaling extraction workflow\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Step 1: Investigate structure\n",
    "    stdf_parsed, raw_stdf = investigate_stdf_structure(stdf_file)\n",
    "    if not stdf_parsed:\n",
    "        return None\n",
    "    \n",
    "    # Step 2: Extract available scaling info\n",
    "    enhanced_df, metadata = extract_available_scaling_info(stdf_file)\n",
    "    if enhanced_df is None:\n",
    "        return None\n",
    "    \n",
    "    # Step 3: Try to access raw test data for more detailed info\n",
    "    raw_test_info = try_access_raw_test_data(stdf_file)\n",
    "    \n",
    "    # Step 4: Apply common scaling patterns\n",
    "    final_df = apply_known_scaling_patterns(enhanced_df)\n",
    "    \n",
    "    # Step 5: Summary\n",
    "    print(\"\\n📊 FINAL RESULTS\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    original_cols = len(stdf_parsed['df'].columns)\n",
    "    final_cols = len(final_df.columns)\n",
    "    new_cols = final_cols - original_cols\n",
    "    \n",
    "    print(f\"Original columns: {original_cols}\")\n",
    "    print(f\"Final columns: {final_cols}\")\n",
    "    print(f\"New columns added: {new_cols}\")\n",
    "    \n",
    "    # Show new columns\n",
    "    all_new_cols = [col for col in final_df.columns if any(suffix in col for suffix in \n",
    "                   ['_units', '_test_name', '_low_limit', '_high_limit', '_description', '_scaled_', '_scale_factor'])]\n",
    "    \n",
    "    print(f\"\\nNew columns created:\")\n",
    "    for i, col in enumerate(all_new_cols[:20]):  # Show first 20\n",
    "        print(f\"  {i+1:2d}. {col}\")\n",
    "    \n",
    "    if len(all_new_cols) > 20:\n",
    "        print(f\"  ... and {len(all_new_cols) - 20} more\")\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your STDF file\n",
    "    stdf_file = \"your_file.stdf\"\n",
    "    \n",
    "    # Run the workflow\n",
    "    try:\n",
    "        result_df = main_scaling_workflow(stdf_file)\n",
    "        \n",
    "        if result_df is not None:\n",
    "            print(f\"\\n✅ SUCCESS! Enhanced DataFrame ready with {len(result_df.columns)} columns\")\n",
    "            \n",
    "            # Optional: Save to CSV\n",
    "            # result_df.to_csv(\"enhanced_stdf_data.csv\", index=False)\n",
    "            # print(\"💾 Saved to enhanced_stdf_data.csv\")\n",
    "            \n",
    "        else:\n",
    "            print(\"\\n❌ Failed to create enhanced DataFrame\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\n💥 Error in main workflow: {e}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
